{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Universal Sentence Encoder + ANN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPFsuvb2TQ5V",
        "colab_type": "code",
        "outputId": "ee372dfa-a71f-4c6c-f366-427ceabe9d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation,Lambda,Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.initializers import glorot_uniform\n",
        "import tensorflow as tf\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leE30OvmVerD",
        "colab_type": "code",
        "outputId": "d83890be-884a-420c-b56b-d9dba944c869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.Module(module_url)\n",
        "\n",
        "# Compute a representation for each message, showing various lengths supported.\n",
        "messages = [\"That band rocks!\", \"That song is really cool.\"]\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  message_embeddings = session.run(embed(messages))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0606 04:35:23.557449 140527414105984 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0606 04:35:35.525181 140527414105984 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0606 04:35:39.465465 140527414105984 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC_XpkGyTT-U",
        "colab_type": "code",
        "outputId": "c1f196ac-c8dc-48c1-c050-1afd5cfd6a81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "def get_dataframe(filename):\n",
        "    lines = open(filename, 'r').read().splitlines()\n",
        "    data = []\n",
        "    for i in range(0, len(lines)):\n",
        "        label = lines[i].split(',')[1]\n",
        "        tweet= lines[i].split(',')[0]\n",
        "        tweet= re.sub('[^A-Za-z0-9 ,\\?\\'\\\"-._\\+\\!/\\`@=;:]+', '', tweet)\n",
        "        data.append([tweet,label])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['tweet', 'label'])\n",
        "    df.label = df.label.astype('category')\n",
        "    return df\n",
        "\n",
        "df_train = get_dataframe('/content/Processed+Data-Train.csv')\n",
        "df_train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ebola symptoms early treatment means a much be...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thinking about telling leadership to make indi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this is nurse 2 is at this very moment being r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fourth doctor dies of ebola in sierra leone af...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ebola is not a death sentence early treatment ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet label\n",
              "0  ebola symptoms early treatment means a much be...     0\n",
              "1  thinking about telling leadership to make indi...     1\n",
              "2  this is nurse 2 is at this very moment being r...     0\n",
              "3  fourth doctor dies of ebola in sierra leone af...     2\n",
              "4  ebola is not a death sentence early treatment ...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SucoRPinTUBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = df_train['tweet'].tolist()\n",
        "train_text = np.array(train_text, dtype=object)[:, np.newaxis] #each tweet becomes a new list in the list\n",
        "\n",
        "train_label = np.asarray(pd.get_dummies(df_train.label), dtype = np.int8)# 1 hot vectors\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbxrIqkrTUEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), \n",
        "    \tsignature=\"default\", as_dict=True)[\"default\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ncnj1mvTUJm",
        "colab_type": "code",
        "outputId": "bcb608e4-445c-4a18-9845-2bdf99db0f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "input_text = Input(shape=(1,), dtype=tf.string)\n",
        "embedding = Lambda(UniversalEmbedding,output_shape=(512,))(input_text)\n",
        "dense = Dense(256, activation='relu')(embedding)\n",
        "dense = Dense(128, activation='relu')(embedding)\n",
        "pred = Dense(7, activation='softmax')(dense)\n",
        "model = Model(inputs=[input_text], outputs=pred)\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "\toptimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0606 04:35:56.631726 140527414105984 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjsn4eg5V-Fy",
        "colab_type": "code",
        "outputId": "7665fe84-c505-43c1-8b6e-b0543395bd74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 66,567\n",
            "Trainable params: 66,567\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R3VeWj3pqag",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myGf2CPLpq0w",
        "colab_type": "code",
        "outputId": "fe927500-b762-4d14-cdb3-35a246ba8d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "def get_dataframe(filename):\n",
        "    lines = open(filename, 'r').read().splitlines()\n",
        "    data = []\n",
        "    for i in range(0, len(lines)):\n",
        "        label = lines[i].split(',')[1]\n",
        "        tweet= lines[i].split(',')[0]\n",
        "        tweet= re.sub('[^A-Za-z0-9 ,\\?\\'\\\"-._\\+\\!/\\`@=;:]+', '', tweet)\n",
        "        data.append([tweet,label])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['tweet', 'label'])\n",
        "    df.label = df.label.astype('category')\n",
        "    return df\n",
        "\n",
        "df_test = get_dataframe('/content/Processed+Data-Test.csv')\n",
        "df_test.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the us centre for disease control cdc issued t...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>breaking employment agency in berlin placed in...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>abuja schools resume screen pupils for symptom...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>people are now apparently faking ebola to get ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>360nobs ebola disease not death sentence says ...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet label\n",
              "0  the us centre for disease control cdc issued t...     6\n",
              "1  breaking employment agency in berlin placed in...     0\n",
              "2  abuja schools resume screen pupils for symptom...     3\n",
              "3  people are now apparently faking ebola to get ...     0\n",
              "4  360nobs ebola disease not death sentence says ...     6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kesOdg5Qpq3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_text = df_test['tweet'].tolist()\n",
        "test_text = np.array(test_text, dtype=object)[:, np.newaxis] #each tweet becomes a new list in the list\n",
        "\n",
        "test_label = np.asarray(pd.get_dummies(df_test.label), dtype = np.int8)# 1 hot vectors\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcu4G5xlp7Q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5zkC3jBpx71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_MfZraLV-Kx",
        "colab_type": "code",
        "outputId": "787b9b10-81af-4829-a391-733ebca74416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2972
        }
      },
      "source": [
        "from keras import backend as K\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.fit(train_text, \n",
        "            train_label,\n",
        "            epochs=80,\n",
        "            batch_size=16)\n",
        "  loss, acc = model.evaluate(test_text,test_label)\n",
        "print()\n",
        "print(\"Test accuracy = \", acc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0606 04:39:44.440982 140527414105984 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "1583/1583 [==============================] - 32s 20ms/step - loss: 1.6984 - acc: 0.3860\n",
            "Epoch 2/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 1.3418 - acc: 0.5351\n",
            "Epoch 3/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 1.1324 - acc: 0.6096\n",
            "Epoch 4/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 1.0107 - acc: 0.6450\n",
            "Epoch 5/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.9254 - acc: 0.6810\n",
            "Epoch 6/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.8778 - acc: 0.7081\n",
            "Epoch 7/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.8286 - acc: 0.7138\n",
            "Epoch 8/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.8059 - acc: 0.7214\n",
            "Epoch 9/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.7750 - acc: 0.7353\n",
            "Epoch 10/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.7514 - acc: 0.7429\n",
            "Epoch 11/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.7262 - acc: 0.7486\n",
            "Epoch 12/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.7162 - acc: 0.7530\n",
            "Epoch 13/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.6963 - acc: 0.7530\n",
            "Epoch 14/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.6830 - acc: 0.7549\n",
            "Epoch 15/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.6724 - acc: 0.7606\n",
            "Epoch 16/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.6516 - acc: 0.7707\n",
            "Epoch 17/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.6468 - acc: 0.7656\n",
            "Epoch 18/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.6252 - acc: 0.7745\n",
            "Epoch 19/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.6072 - acc: 0.7858\n",
            "Epoch 20/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.5999 - acc: 0.7827\n",
            "Epoch 21/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.5865 - acc: 0.7953\n",
            "Epoch 22/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.5841 - acc: 0.7865\n",
            "Epoch 23/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.5632 - acc: 0.8023\n",
            "Epoch 24/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.5584 - acc: 0.8035\n",
            "Epoch 25/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.5528 - acc: 0.8042\n",
            "Epoch 26/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.5348 - acc: 0.8080\n",
            "Epoch 27/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.5208 - acc: 0.8162\n",
            "Epoch 28/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.5132 - acc: 0.8168\n",
            "Epoch 29/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.5082 - acc: 0.8225\n",
            "Epoch 30/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.5010 - acc: 0.8244\n",
            "Epoch 31/80\n",
            "1583/1583 [==============================] - 31s 19ms/step - loss: 0.4882 - acc: 0.8307\n",
            "Epoch 32/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.4809 - acc: 0.8307\n",
            "Epoch 33/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.4709 - acc: 0.8377\n",
            "Epoch 34/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.4611 - acc: 0.8446\n",
            "Epoch 35/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.4510 - acc: 0.8553\n",
            "Epoch 36/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.4404 - acc: 0.8591\n",
            "Epoch 37/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.4328 - acc: 0.8572\n",
            "Epoch 38/80\n",
            "1583/1583 [==============================] - 31s 20ms/step - loss: 0.4224 - acc: 0.8604\n",
            "Epoch 39/80\n",
            "1583/1583 [==============================] - 30s 19ms/step - loss: 0.4146 - acc: 0.8654\n",
            "Epoch 40/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.4132 - acc: 0.8522\n",
            "Epoch 41/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.4041 - acc: 0.8680\n",
            "Epoch 42/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.3947 - acc: 0.8718\n",
            "Epoch 43/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.3805 - acc: 0.8724\n",
            "Epoch 44/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.3738 - acc: 0.8857\n",
            "Epoch 45/80\n",
            "1583/1583 [==============================] - 27s 17ms/step - loss: 0.3678 - acc: 0.8825\n",
            "Epoch 46/80\n",
            "1583/1583 [==============================] - 30s 19ms/step - loss: 0.3676 - acc: 0.8838\n",
            "Epoch 47/80\n",
            "1583/1583 [==============================] - 30s 19ms/step - loss: 0.3631 - acc: 0.8800\n",
            "Epoch 48/80\n",
            "1583/1583 [==============================] - 35s 22ms/step - loss: 0.3439 - acc: 0.8970\n",
            "Epoch 49/80\n",
            "1583/1583 [==============================] - 43s 27ms/step - loss: 0.3393 - acc: 0.8901\n",
            "Epoch 50/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.3339 - acc: 0.8926\n",
            "Epoch 51/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.3230 - acc: 0.9008\n",
            "Epoch 52/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.3195 - acc: 0.9052\n",
            "Epoch 53/80\n",
            "1583/1583 [==============================] - 29s 18ms/step - loss: 0.3074 - acc: 0.9040\n",
            "Epoch 54/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.3053 - acc: 0.9078\n",
            "Epoch 55/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.3007 - acc: 0.9071\n",
            "Epoch 56/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2941 - acc: 0.9103\n",
            "Epoch 57/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2859 - acc: 0.9097\n",
            "Epoch 58/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2848 - acc: 0.9154\n",
            "Epoch 59/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2758 - acc: 0.9122\n",
            "Epoch 60/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2661 - acc: 0.9210\n",
            "Epoch 61/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2574 - acc: 0.9242\n",
            "Epoch 62/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2619 - acc: 0.9280\n",
            "Epoch 63/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2612 - acc: 0.9185\n",
            "Epoch 64/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2485 - acc: 0.9255\n",
            "Epoch 65/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2462 - acc: 0.9349\n",
            "Epoch 66/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2395 - acc: 0.9318\n",
            "Epoch 67/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2312 - acc: 0.9305\n",
            "Epoch 68/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2282 - acc: 0.9337\n",
            "Epoch 69/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2305 - acc: 0.9375\n",
            "Epoch 70/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2221 - acc: 0.9387\n",
            "Epoch 71/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2278 - acc: 0.9324\n",
            "Epoch 72/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2164 - acc: 0.9375\n",
            "Epoch 73/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2073 - acc: 0.9488\n",
            "Epoch 74/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2045 - acc: 0.9413\n",
            "Epoch 75/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.2078 - acc: 0.9469\n",
            "Epoch 76/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.1948 - acc: 0.9514\n",
            "Epoch 77/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.1908 - acc: 0.9539\n",
            "Epoch 78/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.1844 - acc: 0.9507\n",
            "Epoch 79/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.1864 - acc: 0.9539\n",
            "Epoch 80/80\n",
            "1583/1583 [==============================] - 28s 18ms/step - loss: 0.1808 - acc: 0.9526\n",
            "401/401 [==============================] - 8s 21ms/step\n",
            "\n",
            "Test accuracy =  0.5960099752853042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiRK0QTmV-Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPcFnTlqV-S7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbWiPtVNLkTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDZ4OgQAL4OR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}